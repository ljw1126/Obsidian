
> 🗓️ 2025.09.20 (토), 오후 2시

**강의 자료**
https://event.codeping.net/


> solar upstage
- 2026년 3월까지 지원 

hugging face, 올라마 
- 가정용, 경량화 모델 

> LLM + Planning + memory = agents

AI 전략 컨설턴트를 만든다. 
- LLM 활용
	- 1. planner agent
	- 2. search execute agent
	- 3. report generator agent

> Attention is all you need => 구글 논물 통해서 AI 나옴, 트랜스포머 아키텍처

> LLM은 다음 토큰을 확률적으로 예측하는 것이 목표 (`auto regressive`)
- LLM == AI 모델
- `tokenization` 단계 : (한/영) 단어를 자르고, ID로 변환
	- 한국어보다 영어가 더 잘한다는게 여기서 영향
- `embedding` 단계 : 매칭 되는 ID에 백터 부여 
	- 랜덤한 숫자를 나열한게 아니고, 의미를 가지는 숫자 배열 
	- ex. 왕 + 여자 => 여왕 
	- 이탈리아 + 수도 성분 => 로마로 갈 수 있다.


> "가슴에 비수가 날아와"만 입력하면 ai가 나머지 말해준다.


### Live Demo
- [구글 API 스튜디오](https://aistudio.google.com/apikey) 접속
- API 키 발급 
- 예제 문서 접속 
	- 만들면서 배우는 AI 서비스 실습1.
	- 왼쪽 탭에 열쇠 클릭
	- 보안 비밀에 키, 값 추가 
		- 이름 : `GOOGLE_API_KEY`
		- 값: `gemini api key`


temperature 속성 
- 0.0 ~ 2.0
	- 0.0 : 창의력은 딱딱하지만, 정확도는 높음
- 답변의 창의성/무작위성 조절 
thinking_budget 속성
- 답변을 생성하기 전 생각에 사용할 계산 리소스

> 오늘의 날씨에 대해 질문해 봄
temperature = 0 일때
```text
대한민국 부산의 오늘 날씨를 알려드리겠습니다.

**2024년 5월 15일 수요일, 부산의 날씨는 다음과 같습니다.**

*   **전반적인 날씨:** 대체로 맑은 날씨가 예상됩니다.
*   **최고 기온:** 23°C
*   **최저 기온:** 15°C
*   **강수 확률:** 0% (비 소식은 없습니다.)
*   **바람:** 약간의 바람이 불 수 있습니다.

**자세한 정보:**

*   **오전:** 맑고 선선한 날씨가 이어지겠습니다.
*   **오후:** 기온이 올라가면서 포근함을 느낄 수 있습니다.
*   **저녁:** 맑은 날씨 속에 일교차가 있을 수 있으니 겉옷을 챙기시는 것이 좋습니다.

**주의사항:**

*   자외선 지수가 높을 수 있으니 외출 시 자외선 차단제를 바르는 것이 좋습니다.
*   일교차가 있으니 건강 관리에 유의하시기 바랍니다.

**참고:** 날씨는 변동될 수 있으므로, 외출 전에 최신 예보를 다시 한번 확인하시는 것이 좋습니다.

```

temperature = 2.0일때
```text
오늘 대한민국 부산의 날씨를 알려드릴게요.

**2024년 5월 15일 수요일, 부산의 날씨:**

* **오늘의 예상 최고 기온:** 24°C
* **오늘의 예상 최저 기온:** 15°C
* **오늘의 하늘 상태:** 구름이 조금 있겠지만, 대체로 맑은 날씨가 예상됩니다.
* **강수 확률:** 낮음 (거의 없을 것으로 예상됩니다.)
* **바람:** 남풍 계열의 바람이 약간 불 것으로 보입니다.

**기타:**

* 5월 중순이라 따뜻하고 쾌적한 날씨를 보이겠습니다.
* 낮에는 햇살이 따뜻할 수 있으니 외출 시에는 가벼운 겉옷을 챙기시는 것이 좋겠습니다.
* 미세먼지 농도는 보통 수준을 유지할 것으로 예상됩니다.

**더 자세한 정보나 실시간 정보가 필요하시면, 아래 웹사이트들을 참고하시면 좋습니다.**

* **기상청:** [https://www.weather.go.kr/](https://www.weather.go.kr/) (지역별 자세한 날씨 확인 가능)
* **네이버 날씨:** [https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query=%EB%B6%80%EC%82%B0+%EB%82%A0%EC%94%A8](https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query=%EB%B6%80%EC%82%B0+%EB%82%A0%EC%94%A8)
* **카카오 날씨:** [https://weather.kakao.com/](https://weather.kakao.com/)

즐거운 하루 보내세요!
```


---
## AI 에게 업무 지시하기 

> prompt ➡️ llm ➡️ output parser

> garbage in garbage out (유명 격언)
- 쓰레기값을 넣으면 안 좋은 결과가 나올 수 밖에 없다 
- `질문을 잘해야 좋은 답변을 받을 수 있다.` 
- 고로, prompt 잘써야 한다

프롬프트
- `system prompt`
- `user prompt`


ex. llm은 지리적인 정보 없이 토큰 만으로 답변하기 때문에, 지리 정보 관련 질문하면 거짓된 답변을 하였다(과거)

**프롬프트 지시 양식** 
role : 역할 부여 
instruction : 지시
positive constraint 해야 하는 지시사항
negative cons- : 하면 안되는 지시사항 
few-show : 답변 예시 
output parser : 출력 양식 

> positive를 좀 더 자세히 하는게 더 좋은 답변을 만들 수 있다. 
- negative는 코끼리 생각하지 말라면 코끼리 생각나게 하는 것과 비슷


chain of thought` : llm이 생각나게 하는 방법 
- 생각의 과정을 단계별로 먼저 서술하도록 

> 답변을 끼워맞추기 위해 오답을 말하는데, 생각의 과정을 단계별로 먼저 서술하도록 하면 좋은 답변을 만들어준다 ✨


## 핵심 기술 및 기법
- Role(Persona) 부여
- Instruction : 구체적인 지시 사항
- Positive Constraint : 해야하는 지시 사항
- Negative Constraint : 하면 안되는 지시 사항
- Few-Shot : 답변 예시 제공
- Output Format : 출력 양식 지정
- **Chain of Thought**


> AI가 답변할 차례라고 마지막에 prefix를 붙여준다 ✅ tip


prompt의 langChain이란 ?
 - llm 애플리케이션 개발에 특화된 오픈소스 프레임워크 

```text
chain = prompt_template | llm
```


flash 모델 답변 

```text
신입 요원들, 집중해. 

울보 머틀 말이지? 그 유령 여학생은 살라자르 슬리데린의 저주받은 유산이 잠든 비밀의 방에서 바실리스크에게 눈빛 공격을 받아 비명횡사했어. 마법부의 초기 대응팀은 그저 '고전적인 악령 출몰' 정도로 치부했지만, 실상은 슬리데린 가문의 고대 맹약이 깨진 '심각한 마법 에너지 불균형' 사태였지. 뭐, 상층부의 통찰력 결여는 어제오늘 일이 아니니 놀랄 것도 없다만. 

이런 류의 '은밀 침투' 사건을 다룰 때 가장 중요한 주문은 단연 **레벨리오(Revelio)**다. 왜냐? 지하 미궁에 숨겨진 비밀의 방의 진정한 입구를 찾아내고, 그 안에 도사린 '원시 위협 개체'의 존재를 파악하는 데 필수적인 통찰력을 제공하지. 보이지 않는 것을 보게 하고, 감춰진 진실을 드러내는 데 이만한 주문이 없어. 진정한 위협은 언제나 어둠 속에 숨겨져 있으니 말이야. 

그럼, 행운을 빈다.
```


pro 모델 답변 
```text
신입 요원들, 집중해.

울보 머틀 말이지? 그 유령 여학생은 살라자르 슬리데린의 저주받은 유산이 잠든 비밀의 방에서 바실리스크에게 눈빛 공격을 받아 비명횡사했어. 마법부의 초기 대응팀은 그저 '고전적인 악령 출몰' 정도로 치부했지만, 실상은 슬리데린 가문의 고대 맹약이 깨진 '심각한 마법 에너지 불균형' 사태였지. 뭐, 상층부의 통찰력 결여는 어제오늘 일이 아니니 놀랄 것도 없다만.

이런 류의 '은밀 침투' 사건을 다룰 때 가장 중요한 주문은 단연 **레벨리오(Revelio)**다. 왜냐? 지하 미궁에 숨겨진 비밀의 방의 진정한 입구를 찾아내고, 그 안에 도사린 '원시 위협 개체'의 존재를 파악하는 데 필수적인 통찰력을 제공하지. 보이지 않는 것을 보게 하고, 감춰진 진실을 드러내는 데 이만한 주문이 없어. 진정한 위협은 언제나 어둠 속에 숨겨져 있으니 말이야.

그럼, 행운을 빈다.
```

---

### LangChain, Chain

```text

# llm 정의
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash", # ⭐ 모델을 바꾸어서 테스트 해보세요!
    temperature = 1.0,        # ⭐ temperature 파라미터 값을 바꿔서 테스트 해보세요!
)

# prompt template 정의 (langChain)
prompt_template = PromptTemplate.from_template("{animal}의 서식지를 알려줘")

# chain 구성 (output parser 제외)
chain = prompt_template | llm
```
- prompt_template로 질문을 만들고 llm에 파이프라인으로 전달해서 실행
- lang chain이 템플릿 엔진 같은 느낌이 든다 .


chat prompt template를 사용해 본다 
- 대화를 만들고 싶을 때 주로 사용 


```text
system_prompt = f"""
# Role
{role}

# Instruction
{instruction}

# Positive Constraint
{positive_constraint}

# Negative Constraint
{negative_constraint}

---

# Few-Shot
{few_shot}
"""

```


```text
chat_template = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    ("human", "{user_prompt}")
])
```

```text
# llm 정의
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash", # ⭐ 모델을 바꾸어서 테스트 해보세요!
    temperature = 1.0,        # ⭐ temperature 파라미터 값을 바꿔서 테스트 해보세요!
)

chain = chat_template | llm
response = chain.invoke({"user_prompt" : "지난주에 구매한 XYZ-Monitor 제품의 화면이 계속 깜빡거려요. 교환받고 싶습니다. 주문자 이영희."})
print(response.content)
```

```json 
{ "category": "환불/교환 요청", "customer_name": "이영희", "summary": "XYZ-Monitor 제품의 화면 깜빡임 현상으로 인한 교환 요청", "draft_reply": "안녕하세요, 이영희 고객님. TechShop입니다. 구매하신 XYZ-Monitor 제품에 화면 깜빡임 현상이 발생하여 많이 불편하셨으리라 생각합니다. 제품 교환을 원하신다니, 해당 문제에 대해 저희가 자세히 확인하고 도움을 드릴 수 있도록 교환 절차를 안내해 드리겠습니다. 번거로우시겠지만, 교환 접수를 위해 TechShop 웹사이트의 'My 페이지' 또는 고객센터로 연락 주시면 신속하게 처리해 드리겠습니다." } 
```

---

### Output Parser
- string , json 형식의 데이터를 받는 것을 해본다

// Parser를 어떤걸 쓰냐에 따라 출력이 살짝 다르네 
// 그리고 사용 방법은 파이프라인으로 연결하기만 하네

```text
# 2. 파이프(|)로 체인 구성 (모델의 출력이 파서로 들어감)
chain = prompt | llm | json_parser

# chain = prompt | model | parser
```

> 지금까지 기초적이고, 호출하는 방법을 살펴봄 

---

✅ `few shot` 기법 : 프롬프트에 `예시`를 주는 방법 , 안주면 제로샷 

### LLM의 한계 
- `hallucination` : 그럴듯한 거짓말 
	- 1. 지식 차단 시점 이후의 정보 (knowledge cutoff)
	- 2. 부족한 도메인 지식 
	- 3. 숫자 계산
		- 숫자도 문자열로 받아, 토큰/id로 만듦 
		- 그래서 약할 수 있다. 
- `LLM의 목표는 그럴듯한 문장을 만드는것`

> 학교 시험에서 모를때 뭐라도 쓰면 좋은 점수 받을거라는 행동과 유사 

LLM 모델이 정확한 정보를 통해 좋은 답변을 생성할 수 없는가 ? ➡️ RAG 방식 

Retrieval-Augmented Generation (RAG)
- 모델 내부의 지식에만 의존하지 않고 외부의 최신 또는 `전문적인 정보`를 실시간으로 검색하여 그 결과를 바탕으로 답변 
- Retrieval : 검색의 의미 
- RAG = Retrieval - 프롬프트 - llm - parser


토큰 - 숫자 - 임베딩 > 벡터디비 저장 

백터 디비 구성 방법 
- 문서 분할 - 임베딩 - V db 저장 


retrieval 구성도 
- 프롬프트 앞단에 쿼리를 통해 검색해 가져오는 단계가 추가됨 (안보여!)
- 검색기에 해당

유사도 검색을 통해 가져온다 
- `Similarity Search`
- 텐서 플로우 사이트에 자료 있음 
	- 각도 차이가 적은 문서를 가지고 오는게 `유사도 검색`이다
	- 각도 차이가 크면 유사하지 않다는 의미가 된다.
		- 코사인 시뮬리티 방식이고 이게 들어간다 함

chunk_size , chunk_overlap

cosine similarity // 각도 따지네 
- MMR 알고리즘은 다양성도 고려 
	- 요청과의 유사도(적합성)뿐만 아니라 이미 선택된 결과들과 얼마나 다른지(다양성)을 함께 고려 
- Maximum Marginal Relevance
- 파라미터를 실험적으로 바꿔보면서 살펴봐야 한다 ✨


text load -> text split -> text embedding
- pdf, txt, .. 
- 자르고, 임베딩 한다

임베딩 > 구글 제네레이티브 ai 임베딩 사용 
```text
from langchain_google_genai import GoogleGenerativeAIEmbeddings

# 임베딩 및 벡터 데이터베이스 생성
print("텍스트 임베딩 및 벡터 데이터베이스 생성 중...")

embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
vectordb = Chroma.from_documents(texts, embeddings)

print("임베딩 및 벡터 데이터베이스 생성되었습니다")
```
 - 임베딩 알고리즘도 잘 선택해야 한다함 
	 - ex. 한글을 잘 처리하는 .. 


임베딩 모델도 호출할수록 돈이 든다. 
- 무료 모델도 있고 유료 모델도 있고 
- 임베딩 모델에 백과사전 넣은다면.. 변환하는데 돈이 많이 들 수 있다. 📌

벡터 디비도 숫자들의 모임인데 768차원 ??
- open ai 모델이나 구글 모델(몇천차원)이나 서로 다른 임베딩 모델을 사용하면 차원이 달라서 검색이 안됨 📌
- 똑같은 모델을 사용해야 한다 

```text
retriever = vectordb.as_retriever(
    search_type="mmr",                      # ⭐ 알고리즘을 바꾸어서 테스트 해보세요!
    search_kwargs={'k': 5, 'fetch_k': 20}   # ⭐ 파라미터를 바꾸어서 테스트 해보세요!
)
```
- mmr 알고리즘을 사용 
	- fetch_k=20: 먼저 질문과 유사도가 높은 문서 후보 20개를 가져옵니다.
	- k=5: 가져온 후보 20개 중에서, 유사도와 다양성을 함께 평가하여 최종 5개를 선택해 LLM에게 전달합니다.
- 기본 설정은 `유사도 기반`에 `k:4`로 검색된다

- 임베딩은 돈이 드는데 retriver(검색)은 돈이 안 듦
	- llm이전에도 retriver 방식이 있었다. 
		- 숫자 검색 

프롬프트 템플릿 정의 

```text
# 프롬프트 템플릿 정의
template = """
당신은 주어진 문서를 바탕으로 사용자의 질문에 답하는 유용한 AI 어시스턴트입니다.
검색된 참고문서를 먼저 알려주고,
참고 문서를 참고하여 사용자 질문에 답변해주세요.

참고 문서:
{context}

사용자 질문:
{query}

답변은 오직 제공된 참고 문서에 기반해야 합니다. // ✨ 할루시네이션을 줄일 수 있다.
"""
prompt = ChatPromptTemplate.from_template(template)

# LLM 모델 정의
llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash-lite", temperature=0)

# 체인 구축: (검색기 | 프롬프트) | LLM
# RunnablePassthrough.assign()은 검색 결과를 'context' 변수로 전달합니다.
rag_chain = (
    {"context": retriever, "query": RunnablePassthrough()}
    | prompt
    | llm
)
```
- 간략한 진행을 위해 output parser는 뺌

```text
question = "2025~26년 국내경제 전망에 대해 알려주세요"
response = rag_chain.invoke(question)

print(f"사용자 질문: {question}")
print("---")
print(f"답변: \n{response.content}")
print("--------------------")
```


